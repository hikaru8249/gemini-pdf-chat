import os
import streamlit as st
import google.generativeai as genai
from dotenv import load_dotenv
from pypdf import PdfReader

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# APIã‚­ãƒ¼è¨­å®š
api_key = os.environ.get("GOOGLE_API_KEY")
if not api_key:
    st.error("APIã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    st.stop()

genai.configure(api_key=api_key)
# â˜…ã“ã“ã§æˆåŠŸã—ãŸãƒ¢ãƒ‡ãƒ«åã‚’æŒ‡å®š
model = genai.GenerativeModel("gemini-3-flash-preview")

st.title("ğŸ“„ PDF AI Chatbot (Gemini 3)")

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼: PDFã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½ ---
with st.sidebar:
    st.header("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    uploaded_file = st.file_uploader("PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°ï¼†ãƒ‰ãƒ­ãƒƒãƒ—", type=["pdf"])
    
    document_text = ""
    if uploaded_file is not None:
        try:
            # PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
            reader = PdfReader(uploaded_file)
            for page in reader.pages:
                document_text += page.extract_text()
            st.success(f"èª­ã¿è¾¼ã¿å®Œäº†: {len(document_text)}æ–‡å­—")
        except Exception as e:
            st.error(f"èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

# --- ãƒ¡ã‚¤ãƒ³ãƒãƒ£ãƒƒãƒˆç”»é¢ ---

# å±¥æ­´ã®åˆæœŸåŒ–
if "messages" not in st.session_state:
    st.session_state.messages = []

# éå»ã®ä¼šè©±ã‚’è¡¨ç¤º
for message in st.session_state.messages:
    role_show = "assistant" if message["role"] == "model" else "user"
    with st.chat_message(role_show):
        st.markdown(message["content"])

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
if prompt := st.chat_input("ã“ã®PDFã«ã¤ã„ã¦è³ªå•ã—ã¦ã­"):
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’è¡¨ç¤º
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # Geminiã¸ã®æŒ‡ç¤ºï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰ã‚’ä½œæˆ
    # ã“ã“ãŒãƒã‚¤ãƒ³ãƒˆ: PDFã®ä¸­èº«ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸ‹ã‚è¾¼ã¿ã¾ã™
    if document_text:
        full_prompt = f"""
ä»¥ä¸‹ã®ã€Œå‚è€ƒãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã€ã®å†…å®¹ã«åŸºã¥ã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

[å‚è€ƒãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ]
{document_text}

[ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•]
{prompt}
"""
    else:
        # PDFãŒãªã„å ´åˆã¯æ™®é€šã®ãƒãƒ£ãƒƒãƒˆ
        full_prompt = prompt

    # APIã¸ã®é€ä¿¡å±¥æ­´ã‚’ä½œæˆï¼ˆç›´è¿‘ã®ã‚„ã‚Šå–ã‚Šã®ã¿é€ä¿¡ã™ã‚‹ç°¡æ˜“ç‰ˆï¼‰
    gemini_history = []
    # ç›´å‰ã®ä¼šè©±ãŒã‚ã‚Œã°æ–‡è„ˆã¨ã—ã¦è¿½åŠ ï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ã®ãŸã‚æœ€æ–°2å¾€å¾©ç¨‹åº¦æ¨å¥¨ã ãŒã€ä»Šå›ã¯ã‚·ãƒ³ãƒ—ãƒ«ã«ï¼‰
    # ä»Šå›ã¯ã€Œä¸€å•ä¸€ç­”ã€å½¢å¼ã§PDFã®å†…å®¹ã‚’èããŸã‚ã€historyã‚’ä½¿ã‚ãšç›´æ¥ generate_content ã‚’å©ãã¾ã™

    with st.chat_message("assistant"):
        try:
            # stream=True ã§æ–‡å­—ãŒãƒ‘ãƒ©ãƒ‘ãƒ©å‡ºã‚‹ã‚ˆã†ã«ã™ã‚‹
            response_stream = model.generate_content(full_prompt, stream=True)
            
            response_placeholder = st.empty()
            full_response = ""
            
            for chunk in response_stream:
                if chunk.text:
                    full_response += chunk.text
                    response_placeholder.markdown(full_response + "â–Œ")
            
            response_placeholder.markdown(full_response)
            
            # å±¥æ­´ã«ä¿å­˜
            st.session_state.messages.append({"role": "model", "content": full_response})

        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")