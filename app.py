import os
import streamlit as st
from dotenv import load_dotenv
from pypdf import PdfReader

# LlamaIndexã®ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
from llama_index.core import Document, VectorStoreIndex, Settings
from llama_index.llms.gemini import Gemini
from llama_index.embeddings.gemini import GeminiEmbedding

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# 1. APIã‚­ãƒ¼ã®è¨­å®š
api_key = os.environ.get("GOOGLE_API_KEY")
if not api_key:
    st.error("APIã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    st.stop()

# 2. LlamaIndexã®è¨­å®šï¼ˆã“ã“ãŒãƒ—ãƒ­ã®æŠ€ï¼ï¼‰
# LLMï¼ˆå›ç­”ã™ã‚‹é ­è„³ï¼‰ã«Geminiã‚’æŒ‡å®š
Settings.llm = Gemini(
    model="gemini-3-flash-preview", 
    api_key=api_key, 
    temperature=0.3
)
# Embeddingï¼ˆæ¤œç´¢ç”¨ã«æ–‡ç« ã‚’æ•°å€¤åŒ–ã™ã‚‹æ©Ÿèƒ½ï¼‰ã«ã‚‚Geminiã‚’æŒ‡å®š
Settings.embed_model = GeminiEmbedding(
    model_name="models/text-embedding-004", 
    api_key=api_key
)

st.title("ğŸ” Pro RAG Chatbot (LlamaIndex)")

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼: PDFã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ---
with st.sidebar:
    st.header("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç™»éŒ²")
    uploaded_file = st.file_uploader("PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["pdf"])
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼ˆãƒ¡ãƒ¢ãƒªï¼‰ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒã‚ã‚‹ã‹ç¢ºèª
    if "index" not in st.session_state:
        st.session_state.index = None

    if uploaded_file is not None and st.session_state.index is None:
        with st.spinner("AIç”¨æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆä¸­..."):
            try:
                # PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
                reader = PdfReader(uploaded_file)
                text = ""
                for page in reader.pages:
                    text += page.extract_text()
                
                # LlamaIndexç”¨ã®ã€ŒDocumentã€å½¢å¼ã«å¤‰æ›
                documents = [Document(text=text)]
                
                # â˜…ã“ã“ãŒæ ¸å¿ƒï¼ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆ
                # ãƒ†ã‚­ã‚¹ãƒˆã‚’è‡ªå‹•ã§åˆ†å‰²ã—ã€æ•°å€¤åŒ–ã—ã¦æ¤œç´¢ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
                index = VectorStoreIndex.from_documents(documents)
                
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜
                st.session_state.index = index
                st.success("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆå®Œäº†ï¼æ¤œç´¢å¯èƒ½ã§ã™ã€‚")
                
            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

# --- ãƒãƒ£ãƒƒãƒˆç”»é¢ ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# å±¥æ­´è¡¨ç¤º
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"):
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # AIã®å›ç­”ç”Ÿæˆ
    with st.chat_message("assistant"):
        if st.session_state.index is None:
            response_text = "ã¾ãšã¯ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚"
            st.warning(response_text)
        else:
            try:
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½¿ã£ã¦ã€Œæ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã€ã‚’ä½œã‚‹
                query_engine = st.session_state.index.as_query_engine()
                
                # æ¤œç´¢ ï¼‹ å›ç­”ç”Ÿæˆ
                response = query_engine.query(prompt)
                response_text = str(response)
                
                st.markdown(response_text)
            except Exception as e:
                response_text = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
                st.error(response_text)

    st.session_state.messages.append({"role": "assistant", "content": response_text})