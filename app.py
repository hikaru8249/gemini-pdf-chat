import streamlit as st
import os
import tempfile
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.llms.gemini import Gemini
from llama_index.embeddings.gemini import GeminiEmbedding
from dotenv import load_dotenv

# 1. ç’°å¢ƒè¨­å®š
load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(page_title="çˆ†é€Ÿ PDF RAG Chat", page_icon="ğŸš€")
st.title("ğŸš€ çˆ†é€Ÿ PDF RAG Chatbot")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«APIã‚­ãƒ¼å…¥åŠ›æ¬„ï¼ˆå¿µã®ãŸã‚ï¼‰
if not GOOGLE_API_KEY:
    GOOGLE_API_KEY = st.sidebar.text_input("Google API Key", type="password")

if not GOOGLE_API_KEY:
    st.warning("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«(.env)ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€APIã‚­ãƒ¼ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# 2. ãƒ¢ãƒ‡ãƒ«è¨­å®š (ã“ã“ã§Geminiã‚’æŒ‡å)
try:
    # ä»¥å‰å‹•ã„ãŸè¨­å®šï¼ˆgemini-1.5-flash ã¾ãŸã¯ gemini-proï¼‰ã‚’ä½¿ã£ã¦ãã ã•ã„
    Settings.llm = Gemini(
        model="models/gemini-3-flash-preview", 
        api_key=GOOGLE_API_KEY, 
        temperature=0.3
    )
    Settings.embed_model = GeminiEmbedding(
        model_name="models/text-embedding-004", 
        api_key=GOOGLE_API_KEY
    )
except Exception as e:
    st.error(f"ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
    st.stop()

# --- ğŸš€ ã“ã“ãŒçˆ†é€ŸåŒ–ã®ãƒã‚¤ãƒ³ãƒˆï¼ ---
# @st.cache_resource ã‚’ã¤ã‘ã‚‹ã¨ã€ã“ã®é–¢æ•°ã®çµæœãŒãƒ¡ãƒ¢ãƒªã«ä¿å­˜ã•ã‚Œã¾ã™ã€‚
# åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹é™ã‚Šã€2å›ç›®ä»¥é™ã¯ã€Œä¸€ç¬ã€ã§çµ‚ã‚ã‚Šã¾ã™ã€‚
@st.cache_resource(show_spinner=False)
def create_index_from_uploaded_file(uploaded_file):
    with st.spinner("ğŸš€ AIãŒPDFã‚’èª­ã‚“ã§å­¦ç¿’ä¸­...ï¼ˆã“ã‚Œã«ã¯å°‘ã—æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰"):
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_path = tmp_file.name

        # ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
        documents = SimpleDirectoryReader(input_files=[tmp_path]).load_data()
        index = VectorStoreIndex.from_documents(documents)
        
        # æƒé™¤ï¼ˆä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ï¼‰
        os.remove(tmp_path)
        return index

# 3. ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¨ãƒãƒ£ãƒƒãƒˆç”»é¢
uploaded_file = st.file_uploader("PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type=["pdf"])

if uploaded_file:
    # ã“ã“ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ãã®é–¢æ•°ã‚’å‘¼ã³å‡ºã™
    try:
        index = create_index_from_uploaded_file(uploaded_file)
        query_engine = index.as_query_engine()
        st.success("âœ… æº–å‚™å®Œäº†ï¼è³ªå•ã—ã¦ãã ã•ã„ã€‚")

        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–
        if "messages" not in st.session_state:
            st.session_state.messages = []

        # å±¥æ­´ã‚’è¡¨ç¤º
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›å‡¦ç†
        if prompt := st.chat_input("ã“ã®PDFã«ã¤ã„ã¦èã„ã¦ã¿ã¦..."):
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’è¡¨ç¤º
            with st.chat_message("user"):
                st.markdown(prompt)
            st.session_state.messages.append({"role": "user", "content": prompt})

            # AIã®å›ç­”ç”Ÿæˆ
            with st.chat_message("assistant"):
                response = query_engine.query(prompt)
                st.markdown(response.response)
            st.session_state.messages.append({"role": "assistant", "content": response.response})

    except Exception as e:
        st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")